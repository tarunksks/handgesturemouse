{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df832649-10cb-4076-ae2b-381c28d1a895",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyautogui\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import math\n",
    "\n",
    "# Mediapipe hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Screen size for pyautogui (used to scale hand positions to screen size)\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to calculate the distance between two landmarks\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt((landmark1.x - landmark2.x) ** 2 + (landmark1.y - landmark2.y) ** 2)\n",
    "\n",
    "# Mouse action flags\n",
    "is_dragging = False\n",
    "\n",
    "# Function to move mouse based on hand position\n",
    "def move_mouse(index_finger_x, index_finger_y):\n",
    "    screen_x = int(index_finger_x * screen_width)\n",
    "    screen_y = int(index_finger_y * screen_height)\n",
    "    pyautogui.moveTo(screen_x, screen_y)\n",
    "    print(f\"Mouse moved to: ({screen_x}, {screen_y})\")\n",
    "\n",
    "# Function to perform left click\n",
    "def left_click_mouse():\n",
    "    pyautogui.click()\n",
    "    print(\"Left click\")\n",
    "\n",
    "# Function to perform right click\n",
    "def right_click_mouse():\n",
    "    pyautogui.click(button='right')\n",
    "    print(\"Right click\")\n",
    "\n",
    "# Hand gesture detection function\n",
    "def detect_hand_gestures():\n",
    "    global is_dragging\n",
    "\n",
    "    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            # Flip the frame horizontally for natural movement\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Process the image and detect hands\n",
    "            results = hands.process(image)\n",
    "\n",
    "            # Convert the RGB image back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Get the index finger tip and thumb tip landmarks\n",
    "                    index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "                    # Get normalized hand coordinates for index finger tip\n",
    "                    index_finger_x = index_finger_tip.x\n",
    "                    index_finger_y = index_finger_tip.y\n",
    "\n",
    "                    # Calculate the distance between thumb and index finger tip for pinch gesture\n",
    "                    pinch_distance = calculate_distance(index_finger_tip, thumb_tip)\n",
    "\n",
    "                    # Perform left-click if a pinch is detected (thumb close to index finger)\n",
    "                    if pinch_distance < 0.05:  # Adjust the threshold for sensitivity\n",
    "                        if not is_dragging:\n",
    "                            left_click_mouse()\n",
    "                            pyautogui.mouseDown()  # Start dragging (left click)\n",
    "                            is_dragging = True\n",
    "                    else:\n",
    "                        if is_dragging:\n",
    "                            pyautogui.mouseUp()  # Release drag\n",
    "                            is_dragging = False\n",
    "\n",
    "                    # Right-click gesture: use a specific distance threshold to detect right-click (e.g., two fingers pinch)\n",
    "                    middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                    right_click_distance = calculate_distance(index_finger_tip, middle_finger_tip)\n",
    "\n",
    "                    # If right-click gesture detected (index and middle fingers close)\n",
    "                    if right_click_distance < 0.05:  # Adjust sensitivity as needed\n",
    "                        right_click_mouse()\n",
    "\n",
    "                    # Move mouse based on index finger tip position\n",
    "                    move_mouse(index_finger_x, index_finger_y)\n",
    "\n",
    "            # Display the frame in a separate window\n",
    "            cv2.imshow(\"Hand Gesture Detection\", image)\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start hand gesture detection\n",
    "detect_hand_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ac6d48-8cf3-4742-b66c-6ee8e65718ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2577501520.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install opencv-python\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "bash\n",
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54dd4a9d-5a18-4cb3-8e5a-4bc1e41fe436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a348bc-d872-48d4-80ce-b1f05ced8612",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyautogui\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import math\n",
    "\n",
    "# Mediapipe hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Screen size for pyautogui (used to scale hand positions to screen size)\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to calculate the distance between two landmarks\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt((landmark1.x - landmark2.x) ** 2 + (landmark1.y - landmark2.y) ** 2)\n",
    "\n",
    "# Mouse action flags\n",
    "is_dragging = False\n",
    "\n",
    "# Function to move mouse based on hand position\n",
    "def move_mouse(index_finger_x, index_finger_y):\n",
    "    screen_x = int(index_finger_x * screen_width)\n",
    "    screen_y = int(index_finger_y * screen_height)\n",
    "    pyautogui.moveTo(screen_x, screen_y)\n",
    "    print(f\"Mouse moved to: ({screen_x}, {screen_y})\")\n",
    "\n",
    "# Function to perform left click\n",
    "def left_click_mouse():\n",
    "    pyautogui.click()\n",
    "    print(\"Left click\")\n",
    "\n",
    "# Function to perform right click\n",
    "def right_click_mouse():\n",
    "    pyautogui.click(button='right')\n",
    "    print(\"Right click\")\n",
    "\n",
    "# Hand gesture detection function\n",
    "def detect_hand_gestures():\n",
    "    global is_dragging\n",
    "\n",
    "    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            # Flip the frame horizontally for natural movement\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Process the image and detect hands\n",
    "            results = hands.process(image)\n",
    "\n",
    "            # Convert the RGB image back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Get the index finger tip and thumb tip landmarks\n",
    "                    index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "                    # Get normalized hand coordinates for index finger tip\n",
    "                    index_finger_x = index_finger_tip.x\n",
    "                    index_finger_y = index_finger_tip.y\n",
    "\n",
    "                    # Calculate the distance between thumb and index finger tip for pinch gesture\n",
    "                    pinch_distance = calculate_distance(index_finger_tip, thumb_tip)\n",
    "\n",
    "                    # Perform left-click if a pinch is detected (thumb close to index finger)\n",
    "                    if pinch_distance < 0.05:  # Adjust the threshold for sensitivity\n",
    "                        if not is_dragging:\n",
    "                            left_click_mouse()\n",
    "                            pyautogui.mouseDown()  # Start dragging (left click)\n",
    "                            is_dragging = True\n",
    "                    else:\n",
    "                        if is_dragging:\n",
    "                            pyautogui.mouseUp()  # Release drag\n",
    "                            is_dragging = False\n",
    "\n",
    "                    # Right-click gesture: use a specific distance threshold to detect right-click (e.g., two fingers pinch)\n",
    "                    middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                    right_click_distance = calculate_distance(index_finger_tip, middle_finger_tip)\n",
    "\n",
    "                    # If right-click gesture detected (index and middle fingers close)\n",
    "                    if right_click_distance < 0.05:  # Adjust sensitivity as needed\n",
    "                        right_click_mouse()\n",
    "\n",
    "                    # Move mouse based on index finger tip position\n",
    "                    move_mouse(index_finger_x, index_finger_y)\n",
    "\n",
    "            # Display the frame in a separate window\n",
    "            cv2.imshow(\"Hand Gesture Detection\", image)\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start hand gesture detection\n",
    "detect_hand_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a990d162-9709-4e49-be12-116a8e89bda4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1834080681.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install mediapipe\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "bash\n",
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77465ad3-281d-45ab-8edd-3a6d9ddf9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.18-cp312-cp312-macosx_11_0_universal2.whl.metadata (9.7 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Downloading jaxlib-0.4.35-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.8.4)\n",
      "Requirement already satisfied: numpy<2 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.1-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.4 kB)\n",
      "Collecting sentencepiece (from mediapipe)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Collecting ml-dtypes>=0.4.0 (from jax->mediapipe)\n",
      "  Downloading ml_dtypes-0.5.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Collecting opt-einsum (from jax->mediapipe)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Downloading mediapipe-0.10.18-cp312-cp312-macosx_11_0_universal2.whl (49.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Downloading sounddevice-0.5.1-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.35-cp312-cp312-macosx_11_0_arm64.whl (68.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (63.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.0-cp312-cp312-macosx_10_9_universal2.whl (750 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.2/750.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: sentencepiece, flatbuffers, protobuf, opt-einsum, opencv-contrib-python, ml-dtypes, absl-py, sounddevice, jaxlib, jax, mediapipe\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed absl-py-2.1.0 flatbuffers-24.3.25 jax-0.4.35 jaxlib-0.4.35 mediapipe-0.10.18 ml-dtypes-0.5.0 opencv-contrib-python-4.10.0.84 opt-einsum-3.4.0 protobuf-4.25.5 sentencepiece-0.2.0 sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f8a65d-18f0-402a-b8f7-bc8c494b9ed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyautogui'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyautogui\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Mediapipe hands setup\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyautogui'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import math\n",
    "\n",
    "# Mediapipe hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Screen size for pyautogui (used to scale hand positions to screen size)\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to calculate the distance between two landmarks\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt((landmark1.x - landmark2.x) ** 2 + (landmark1.y - landmark2.y) ** 2)\n",
    "\n",
    "# Mouse action flags\n",
    "is_dragging = False\n",
    "\n",
    "# Function to move mouse based on hand position\n",
    "def move_mouse(index_finger_x, index_finger_y):\n",
    "    screen_x = int(index_finger_x * screen_width)\n",
    "    screen_y = int(index_finger_y * screen_height)\n",
    "    pyautogui.moveTo(screen_x, screen_y)\n",
    "    print(f\"Mouse moved to: ({screen_x}, {screen_y})\")\n",
    "\n",
    "# Function to perform left click\n",
    "def left_click_mouse():\n",
    "    pyautogui.click()\n",
    "    print(\"Left click\")\n",
    "\n",
    "# Function to perform right click\n",
    "def right_click_mouse():\n",
    "    pyautogui.click(button='right')\n",
    "    print(\"Right click\")\n",
    "\n",
    "# Hand gesture detection function\n",
    "def detect_hand_gestures():\n",
    "    global is_dragging\n",
    "\n",
    "    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            # Flip the frame horizontally for natural movement\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Process the image and detect hands\n",
    "            results = hands.process(image)\n",
    "\n",
    "            # Convert the RGB image back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Get the index finger tip and thumb tip landmarks\n",
    "                    index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "                    # Get normalized hand coordinates for index finger tip\n",
    "                    index_finger_x = index_finger_tip.x\n",
    "                    index_finger_y = index_finger_tip.y\n",
    "\n",
    "                    # Calculate the distance between thumb and index finger tip for pinch gesture\n",
    "                    pinch_distance = calculate_distance(index_finger_tip, thumb_tip)\n",
    "\n",
    "                    # Perform left-click if a pinch is detected (thumb close to index finger)\n",
    "                    if pinch_distance < 0.05:  # Adjust the threshold for sensitivity\n",
    "                        if not is_dragging:\n",
    "                            left_click_mouse()\n",
    "                            pyautogui.mouseDown()  # Start dragging (left click)\n",
    "                            is_dragging = True\n",
    "                    else:\n",
    "                        if is_dragging:\n",
    "                            pyautogui.mouseUp()  # Release drag\n",
    "                            is_dragging = False\n",
    "\n",
    "                    # Right-click gesture: use a specific distance threshold to detect right-click (e.g., two fingers pinch)\n",
    "                    middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                    right_click_distance = calculate_distance(index_finger_tip, middle_finger_tip)\n",
    "\n",
    "                    # If right-click gesture detected (index and middle fingers close)\n",
    "                    if right_click_distance < 0.05:  # Adjust sensitivity as needed\n",
    "                        right_click_mouse()\n",
    "\n",
    "                    # Move mouse based on index finger tip position\n",
    "                    move_mouse(index_finger_x, index_finger_y)\n",
    "\n",
    "            # Display the frame in a separate window\n",
    "            cv2.imshow(\"Hand Gesture Detection\", image)\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start hand gesture detection\n",
    "detect_hand_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9217a5d-e8ed-4c11-98db-9ee3fd2e95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogui\n",
      "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m720.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pymsgbox (from pyautogui)\n",
      "  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytweening>=1.0.4 (from pyautogui)\n",
      "  Downloading pytweening-1.2.0.tar.gz (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.2/171.2 kB\u001b[0m \u001b[31m891.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyscreeze>=0.1.21 (from pyautogui)\n",
      "  Downloading pyscreeze-1.0.1.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygetwindow>=0.0.5 (from pyautogui)\n",
      "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mouseinfo (from pyautogui)\n",
      "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyobjc-core in /opt/anaconda3/lib/python3.12/site-packages (from pyautogui) (10.1)\n",
      "Collecting pyobjc-framework-quartz (from pyautogui)\n",
      "  Downloading pyobjc_framework_Quartz-10.3.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui)\n",
      "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rubicon-objc (from mouseinfo->pyautogui)\n",
      "  Downloading rubicon_objc-0.4.9-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pyperclip (from mouseinfo->pyautogui)\n",
      "  Downloading pyperclip-1.9.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyobjc-core (from pyautogui)\n",
      "  Downloading pyobjc_core-10.3.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-Cocoa>=10.3.1 (from pyobjc-framework-quartz->pyautogui)\n",
      "  Downloading pyobjc_framework_Cocoa-10.3.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Downloading pyobjc_framework_Quartz-10.3.1-cp312-cp312-macosx_10_9_universal2.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.2/227.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_core-10.3.1-cp312-cp312-macosx_10_9_universal2.whl (825 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m826.0/826.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_Cocoa-10.3.1-cp312-cp312-macosx_10_9_universal2.whl (396 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rubicon_objc-0.4.9-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m754.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyautogui, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, pyperclip, pyrect\n",
      "  Building wheel for pyautogui (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyautogui: filename=PyAutoGUI-0.9.54-py3-none-any.whl size=37577 sha256=f4770d6a346c32a468b7a5f8ce17dd057b5c993d724d268117e5f801b036f3cc\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/d9/d6/47/04075995b093ecc87c212c9a3dbd34e59456c6fe504d65c3e4\n",
      "  Building wheel for pygetwindow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11063 sha256=6c57a208686945c273ee63ec2803ca495e71893921eaae634d266fe20794f08e\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/b3/39/81/34dd7a2eca5f885f1f6e2796761970daf66a2d98ac1904f5f4\n",
      "  Building wheel for pyscreeze (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyscreeze: filename=PyScreeze-1.0.1-py3-none-any.whl size=14365 sha256=410422ac1280f6818f62e21344c0b1bcd107703e34b0d66ad3fd199d86233a34\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/cd/3a/c2/7f2839239a069aa3c9564f6777cbb29d733720ef673f104f0d\n",
      "  Building wheel for pytweening (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytweening: filename=pytweening-1.2.0-py3-none-any.whl size=8009 sha256=0c6916f3d134879e4252df479d156cc6da896a591e78aab6cd4e4058f4aff217\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/23/d5/13/4e9bdadbfe3c78e47c675e7410c0eed2fbb63c5ea6cf1b40e7\n",
      "  Building wheel for mouseinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10889 sha256=18987a976a8bbc6745527d8448d15968f30fccaa4763fdadd859544c9b9bcf09\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/b1/9b/f3/08650eb7f00af32f07789f3c6a101e0d7fc762b9891ae843bb\n",
      "  Building wheel for pymsgbox (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pymsgbox: filename=PyMsgBox-1.0.9-py3-none-any.whl size=7405 sha256=4e0152bf170f94a68c65dca4d0b7034e7bc801a4e970ac1a9cf459c0fa1bf0a0\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/55/e7/aa/239163543708d1e15c3d9a1b89dbfe3954b0929a6df2951b83\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.9.0-py3-none-any.whl size=11002 sha256=b0ecc98be1f02580d75bea2c641b4c7bce8fd71ec0d8bd3eaadeb111f7339064\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/e0/e8/fc/8ab8aa326e33bc066ccd5f3ca9646eab4299881af933f94f09\n",
      "  Building wheel for pyrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11179 sha256=fa2a67f805513ddac89ef1bce7dd5468db2f9bad5b7ec87a7d619882ea45f33c\n",
      "  Stored in directory: /Users/usufahmed/Library/Caches/pip/wheels/0b/1e/d7/0c74bd8f60b39c14d84e307398786002aa7ddc905927cc03c5\n",
      "Successfully built pyautogui pygetwindow pyscreeze pytweening mouseinfo pymsgbox pyperclip pyrect\n",
      "Installing collected packages: pytweening, pyscreeze, pyrect, pyperclip, pymsgbox, rubicon-objc, pyobjc-core, pygetwindow, pyobjc-framework-Cocoa, mouseinfo, pyobjc-framework-quartz, pyautogui\n",
      "  Attempting uninstall: pyobjc-core\n",
      "    Found existing installation: pyobjc-core 10.1\n",
      "    Uninstalling pyobjc-core-10.1:\n",
      "      Successfully uninstalled pyobjc-core-10.1\n",
      "  Attempting uninstall: pyobjc-framework-Cocoa\n",
      "    Found existing installation: pyobjc-framework-Cocoa 10.1\n",
      "    Uninstalling pyobjc-framework-Cocoa-10.1:\n",
      "      Successfully uninstalled pyobjc-framework-Cocoa-10.1\n",
      "Successfully installed mouseinfo-0.1.3 pyautogui-0.9.54 pygetwindow-0.0.9 pymsgbox-1.0.9 pyobjc-core-10.3.1 pyobjc-framework-Cocoa-10.3.1 pyobjc-framework-quartz-10.3.1 pyperclip-1.9.0 pyrect-0.2.0 pyscreeze-1.0.1 pytweening-1.2.0 rubicon-objc-0.4.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7cd5fb-a66f-44dd-a6c0-aec84055dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: not authorized to capture video (status 0), requesting...\n",
      "OpenCV: camera failed to properly initialize!\n",
      "[ WARN:0@9.750] global cap.cpp:323 open VIDEOIO(OBSENSOR): raised unknown C++ exception!\n",
      "\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731307823.128487  501734 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/11 12:20:23.042361][info][501734][Context.cpp:69] Context created with config: default config!\n",
      "[11/11 12:20:23.042376][info][501734][Context.cpp:74] Context work_dir=/Users/usufahmed\n",
      "[11/11 12:20:23.042377][info][501734][Context.cpp:77] \t- SDK version: 1.9.4\n",
      "[11/11 12:20:23.042378][info][501734][Context.cpp:78] \t- SDK stage version: main\n",
      "[11/11 12:20:23.042380][info][501734][Context.cpp:82] get config EnumerateNetDevice:false\n",
      "[11/11 12:20:23.042381][info][501734][MacPal.cpp:36] createObPal: create MacPal!\n",
      "[11/11 12:20:23.043539][info][501734][MacPal.cpp:104] Create PollingDeviceWatcher!\n",
      "[11/11 12:20:23.043545][info][501734][DeviceManager.cpp:15] Current found device(s): (0)\n",
      "[11/11 12:20:23.043547][info][501734][Pipeline.cpp:15] Try to create pipeline with default device.\n",
      "[11/11 12:20:23.043548][warning][501734][ObException.cpp:5] No device found, fail to create pipeline!\n",
      "[11/11 12:20:23.044936][info][501734][Context.cpp:90] Context destroyed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1731307823.149091  502175 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731307823.155287  502175 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import math\n",
    "\n",
    "# Mediapipe hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Screen size for pyautogui (used to scale hand positions to screen size)\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to calculate the distance between two landmarks\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt((landmark1.x - landmark2.x) ** 2 + (landmark1.y - landmark2.y) ** 2)\n",
    "\n",
    "# Mouse action flags\n",
    "is_dragging = False\n",
    "\n",
    "# Function to move mouse based on hand position\n",
    "def move_mouse(index_finger_x, index_finger_y):\n",
    "    screen_x = int(index_finger_x * screen_width)\n",
    "    screen_y = int(index_finger_y * screen_height)\n",
    "    pyautogui.moveTo(screen_x, screen_y)\n",
    "    print(f\"Mouse moved to: ({screen_x}, {screen_y})\")\n",
    "\n",
    "# Function to perform left click\n",
    "def left_click_mouse():\n",
    "    pyautogui.click()\n",
    "    print(\"Left click\")\n",
    "\n",
    "# Function to perform right click\n",
    "def right_click_mouse():\n",
    "    pyautogui.click(button='right')\n",
    "    print(\"Right click\")\n",
    "\n",
    "# Hand gesture detection function\n",
    "def detect_hand_gestures():\n",
    "    global is_dragging\n",
    "\n",
    "    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            # Flip the frame horizontally for natural movement\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Process the image and detect hands\n",
    "            results = hands.process(image)\n",
    "\n",
    "            # Convert the RGB image back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Get the index finger tip and thumb tip landmarks\n",
    "                    index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "                    # Get normalized hand coordinates for index finger tip\n",
    "                    index_finger_x = index_finger_tip.x\n",
    "                    index_finger_y = index_finger_tip.y\n",
    "\n",
    "                    # Calculate the distance between thumb and index finger tip for pinch gesture\n",
    "                    pinch_distance = calculate_distance(index_finger_tip, thumb_tip)\n",
    "\n",
    "                    # Perform left-click if a pinch is detected (thumb close to index finger)\n",
    "                    if pinch_distance < 0.05:  # Adjust the threshold for sensitivity\n",
    "                        if not is_dragging:\n",
    "                            left_click_mouse()\n",
    "                            pyautogui.mouseDown()  # Start dragging (left click)\n",
    "                            is_dragging = True\n",
    "                    else:\n",
    "                        if is_dragging:\n",
    "                            pyautogui.mouseUp()  # Release drag\n",
    "                            is_dragging = False\n",
    "\n",
    "                    # Right-click gesture: use a specific distance threshold to detect right-click (e.g., two fingers pinch)\n",
    "                    middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                    right_click_distance = calculate_distance(index_finger_tip, middle_finger_tip)\n",
    "\n",
    "                    # If right-click gesture detected (index and middle fingers close)\n",
    "                    if right_click_distance < 0.05:  # Adjust sensitivity as needed\n",
    "                        right_click_mouse()\n",
    "\n",
    "                    # Move mouse based on index finger tip position\n",
    "                    move_mouse(index_finger_x, index_finger_y)\n",
    "\n",
    "            # Display the frame in a separate window\n",
    "            cv2.imshow(\"Hand Gesture Detection\", image)\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start hand gesture detection\n",
    "detect_hand_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466249e5-27b7-4002-95e3-3fb05e1b6b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731307874.313269  501734 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "W0000 00:00:1731307874.326204  502876 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731307874.332803  502876 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-11-11 12:21:14.893 python[10134:501734] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-11-11 12:21:14.893 python[10134:501734] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "W0000 00:00:1731307878.429169  502881 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse moved to: (555, 232)\n",
      "Mouse moved to: (631, 285)\n",
      "Mouse moved to: (634, 305)\n",
      "Mouse moved to: (634, 292)\n",
      "Mouse moved to: (646, 273)\n",
      "Mouse moved to: (669, 276)\n",
      "Mouse moved to: (698, 273)\n",
      "Mouse moved to: (693, 274)\n",
      "Mouse moved to: (689, 269)\n",
      "Mouse moved to: (694, 267)\n",
      "Mouse moved to: (695, 262)\n",
      "Mouse moved to: (694, 260)\n",
      "Mouse moved to: (692, 261)\n",
      "Mouse moved to: (695, 266)\n",
      "Mouse moved to: (694, 264)\n",
      "Mouse moved to: (692, 264)\n",
      "Mouse moved to: (684, 266)\n",
      "Mouse moved to: (681, 262)\n",
      "Mouse moved to: (676, 260)\n",
      "Mouse moved to: (672, 260)\n",
      "Mouse moved to: (671, 254)\n",
      "Mouse moved to: (660, 251)\n",
      "Mouse moved to: (660, 258)\n",
      "Mouse moved to: (648, 248)\n",
      "Mouse moved to: (637, 256)\n",
      "Mouse moved to: (639, 263)\n",
      "Mouse moved to: (652, 290)\n",
      "Mouse moved to: (676, 316)\n",
      "Mouse moved to: (645, 255)\n",
      "Mouse moved to: (681, 219)\n",
      "Mouse moved to: (716, 218)\n",
      "Mouse moved to: (737, 216)\n",
      "Mouse moved to: (736, 208)\n",
      "Mouse moved to: (737, 211)\n",
      "Mouse moved to: (736, 217)\n",
      "Mouse moved to: (734, 220)\n",
      "Mouse moved to: (721, 218)\n",
      "Mouse moved to: (716, 218)\n",
      "Mouse moved to: (707, 216)\n",
      "Mouse moved to: (705, 217)\n",
      "Mouse moved to: (705, 215)\n",
      "Mouse moved to: (702, 214)\n",
      "Mouse moved to: (698, 213)\n",
      "Mouse moved to: (691, 217)\n",
      "Mouse moved to: (650, 209)\n",
      "Mouse moved to: (667, 230)\n",
      "Mouse moved to: (673, 254)\n",
      "Mouse moved to: (672, 264)\n",
      "Mouse moved to: (660, 260)\n",
      "Mouse moved to: (652, 262)\n",
      "Mouse moved to: (661, 266)\n",
      "Mouse moved to: (665, 258)\n",
      "Mouse moved to: (670, 258)\n",
      "Mouse moved to: (654, 247)\n",
      "Mouse moved to: (654, 239)\n",
      "Mouse moved to: (657, 241)\n",
      "Mouse moved to: (653, 230)\n",
      "Mouse moved to: (657, 237)\n",
      "Mouse moved to: (641, 230)\n",
      "Mouse moved to: (614, 229)\n",
      "Mouse moved to: (615, 277)\n",
      "Mouse moved to: (460, 294)\n",
      "Mouse moved to: (286, 388)\n",
      "Mouse moved to: (110, 289)\n",
      "Right click\n",
      "Mouse moved to: (600, 706)\n",
      "Left click\n",
      "Right click\n",
      "Mouse moved to: (581, 742)\n",
      "Right click\n",
      "Mouse moved to: (614, 692)\n",
      "Mouse moved to: (653, 663)\n",
      "Mouse moved to: (648, 637)\n",
      "Mouse moved to: (659, 628)\n",
      "Mouse moved to: (649, 645)\n",
      "Mouse moved to: (603, 671)\n",
      "Right click\n",
      "Mouse moved to: (619, 676)\n",
      "Right click\n",
      "Mouse moved to: (756, 642)\n",
      "Mouse moved to: (629, 565)\n",
      "Mouse moved to: (629, 442)\n",
      "Mouse moved to: (612, 360)\n",
      "Mouse moved to: (612, 342)\n",
      "Mouse moved to: (623, 344)\n",
      "Mouse moved to: (624, 335)\n",
      "Mouse moved to: (651, 333)\n",
      "Mouse moved to: (744, 352)\n",
      "Mouse moved to: (849, 375)\n",
      "Mouse moved to: (929, 403)\n",
      "Mouse moved to: (1000, 425)\n",
      "Mouse moved to: (1040, 442)\n",
      "Mouse moved to: (1032, 452)\n",
      "Mouse moved to: (985, 425)\n",
      "Mouse moved to: (902, 369)\n",
      "Mouse moved to: (815, 321)\n",
      "Mouse moved to: (741, 295)\n",
      "Mouse moved to: (680, 269)\n",
      "Mouse moved to: (648, 276)\n",
      "Mouse moved to: (627, 282)\n",
      "Mouse moved to: (623, 297)\n",
      "Mouse moved to: (615, 299)\n",
      "Mouse moved to: (609, 307)\n",
      "Mouse moved to: (610, 304)\n",
      "Mouse moved to: (612, 313)\n",
      "Mouse moved to: (600, 306)\n",
      "Mouse moved to: (593, 298)\n",
      "Mouse moved to: (625, 300)\n",
      "Mouse moved to: (677, 306)\n",
      "Mouse moved to: (692, 295)\n",
      "Mouse moved to: (689, 280)\n",
      "Mouse moved to: (696, 285)\n",
      "Mouse moved to: (702, 282)\n",
      "Mouse moved to: (681, 269)\n",
      "Mouse moved to: (665, 272)\n",
      "Mouse moved to: (644, 263)\n",
      "Mouse moved to: (700, 286)\n",
      "Mouse moved to: (684, 293)\n",
      "Mouse moved to: (697, 308)\n",
      "Mouse moved to: (700, 307)\n",
      "Mouse moved to: (699, 305)\n",
      "Mouse moved to: (695, 301)\n",
      "Mouse moved to: (692, 297)\n",
      "Mouse moved to: (698, 304)\n",
      "Mouse moved to: (702, 333)\n",
      "Left click\n",
      "Mouse moved to: (689, 404)\n",
      "Mouse moved to: (683, 311)\n",
      "Mouse moved to: (688, 315)\n",
      "Mouse moved to: (688, 315)\n",
      "Mouse moved to: (686, 312)\n",
      "Mouse moved to: (687, 313)\n",
      "Mouse moved to: (689, 318)\n",
      "Mouse moved to: (686, 319)\n",
      "Mouse moved to: (669, 326)\n",
      "Mouse moved to: (653, 334)\n",
      "Mouse moved to: (624, 417)\n",
      "Right click\n",
      "Mouse moved to: (920, 594)\n",
      "Right click\n",
      "Mouse moved to: (862, 506)\n",
      "Right click\n",
      "Mouse moved to: (856, 507)\n",
      "Right click\n",
      "Mouse moved to: (862, 499)\n",
      "Right click\n",
      "Mouse moved to: (855, 498)\n",
      "Right click\n",
      "Mouse moved to: (750, 425)\n",
      "Right click\n",
      "Mouse moved to: (628, 366)\n",
      "Right click\n",
      "Mouse moved to: (512, 308)\n",
      "Right click\n",
      "Mouse moved to: (418, 284)\n",
      "Right click\n",
      "Mouse moved to: (369, 233)\n",
      "Right click\n",
      "Mouse moved to: (345, 209)\n",
      "Right click\n",
      "Mouse moved to: (357, 203)\n",
      "Right click\n",
      "Mouse moved to: (527, 419)\n",
      "Right click\n",
      "Mouse moved to: (634, 573)\n",
      "Right click\n",
      "Mouse moved to: (744, 574)\n",
      "Right click\n",
      "Mouse moved to: (892, 558)\n",
      "Right click\n",
      "Mouse moved to: (945, 528)\n",
      "Right click\n",
      "Mouse moved to: (932, 538)\n",
      "Right click\n",
      "Mouse moved to: (966, 629)\n",
      "Right click\n",
      "Mouse moved to: (1017, 651)\n",
      "Right click\n",
      "Mouse moved to: (1028, 657)\n",
      "Right click\n",
      "Mouse moved to: (1052, 718)\n",
      "Right click\n",
      "Mouse moved to: (1045, 749)\n",
      "Right click\n",
      "Mouse moved to: (1035, 747)\n",
      "Right click\n",
      "Mouse moved to: (1033, 730)\n",
      "Right click\n",
      "Mouse moved to: (1016, 728)\n",
      "Right click\n",
      "Mouse moved to: (1013, 729)\n",
      "Right click\n",
      "Mouse moved to: (1010, 726)\n",
      "Right click\n",
      "Mouse moved to: (968, 729)\n",
      "Right click\n",
      "Mouse moved to: (789, 692)\n",
      "Right click\n",
      "Mouse moved to: (804, 684)\n",
      "Right click\n",
      "Mouse moved to: (818, 673)\n",
      "Right click\n",
      "Mouse moved to: (810, 673)\n",
      "Right click\n",
      "Mouse moved to: (812, 677)\n",
      "Right click\n",
      "Mouse moved to: (809, 678)\n",
      "Right click\n",
      "Mouse moved to: (807, 681)\n",
      "Right click\n",
      "Mouse moved to: (802, 680)\n",
      "Mouse moved to: (773, 650)\n",
      "Right click\n",
      "Mouse moved to: (753, 647)\n",
      "Right click\n",
      "Mouse moved to: (751, 647)\n",
      "Right click\n",
      "Mouse moved to: (757, 651)\n",
      "Right click\n",
      "Mouse moved to: (761, 648)\n",
      "Right click\n",
      "Mouse moved to: (760, 652)\n",
      "Right click\n",
      "Mouse moved to: (759, 652)\n",
      "Mouse moved to: (777, 659)\n",
      "Left click\n",
      "Mouse moved to: (881, 773)\n",
      "Mouse moved to: (785, 637)\n",
      "Mouse moved to: (784, 637)\n",
      "Mouse moved to: (784, 639)\n",
      "Mouse moved to: (788, 639)\n",
      "Mouse moved to: (789, 639)\n",
      "Mouse moved to: (840, 700)\n",
      "Mouse moved to: (825, 649)\n",
      "Mouse moved to: (800, 649)\n",
      "Mouse moved to: (795, 653)\n",
      "Mouse moved to: (799, 656)\n",
      "Mouse moved to: (801, 659)\n",
      "Mouse moved to: (803, 659)\n",
      "Mouse moved to: (816, 664)\n",
      "Mouse moved to: (890, 665)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import math\n",
    "\n",
    "# Mediapipe hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Screen size for pyautogui (used to scale hand positions to screen size)\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to calculate the distance between two landmarks\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt((landmark1.x - landmark2.x) ** 2 + (landmark1.y - landmark2.y) ** 2)\n",
    "\n",
    "# Mouse action flags\n",
    "is_dragging = False\n",
    "\n",
    "# Function to move mouse based on hand position\n",
    "def move_mouse(index_finger_x, index_finger_y):\n",
    "    screen_x = int(index_finger_x * screen_width)\n",
    "    screen_y = int(index_finger_y * screen_height)\n",
    "    pyautogui.moveTo(screen_x, screen_y)\n",
    "    print(f\"Mouse moved to: ({screen_x}, {screen_y})\")\n",
    "\n",
    "# Function to perform left click\n",
    "def left_click_mouse():\n",
    "    pyautogui.click()\n",
    "    print(\"Left click\")\n",
    "\n",
    "# Function to perform right click\n",
    "def right_click_mouse():\n",
    "    pyautogui.click(button='right')\n",
    "    print(\"Right click\")\n",
    "\n",
    "# Hand gesture detection function\n",
    "def detect_hand_gestures():\n",
    "    global is_dragging\n",
    "\n",
    "    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            # Flip the frame horizontally for natural movement\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Process the image and detect hands\n",
    "            results = hands.process(image)\n",
    "\n",
    "            # Convert the RGB image back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Get the index finger tip and thumb tip landmarks\n",
    "                    index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "                    # Get normalized hand coordinates for index finger tip\n",
    "                    index_finger_x = index_finger_tip.x\n",
    "                    index_finger_y = index_finger_tip.y\n",
    "\n",
    "                    # Calculate the distance between thumb and index finger tip for pinch gesture\n",
    "                    pinch_distance = calculate_distance(index_finger_tip, thumb_tip)\n",
    "\n",
    "                    # Perform left-click if a pinch is detected (thumb close to index finger)\n",
    "                    if pinch_distance < 0.05:  # Adjust the threshold for sensitivity\n",
    "                        if not is_dragging:\n",
    "                            left_click_mouse()\n",
    "                            pyautogui.mouseDown()  # Start dragging (left click)\n",
    "                            is_dragging = True\n",
    "                    else:\n",
    "                        if is_dragging:\n",
    "                            pyautogui.mouseUp()  # Release drag\n",
    "                            is_dragging = False\n",
    "\n",
    "                    # Right-click gesture: use a specific distance threshold to detect right-click (e.g., two fingers pinch)\n",
    "                    middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                    right_click_distance = calculate_distance(index_finger_tip, middle_finger_tip)\n",
    "\n",
    "                    # If right-click gesture detected (index and middle fingers close)\n",
    "                    if right_click_distance < 0.05:  # Adjust sensitivity as needed\n",
    "                        right_click_mouse()\n",
    "\n",
    "                    # Move mouse based on index finger tip position\n",
    "                    move_mouse(index_finger_x, index_finger_y)\n",
    "\n",
    "            # Display the frame in a separate window\n",
    "            cv2.imshow(\"Hand Gesture Detection\", image)\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start hand gesture detection\n",
    "detect_hand_gestures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a38ba-7131-488c-815e-5f97719bc0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
